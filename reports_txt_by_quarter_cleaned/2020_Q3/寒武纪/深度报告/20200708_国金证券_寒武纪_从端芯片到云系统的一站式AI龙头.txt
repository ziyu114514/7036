市场价格（人民币）：  元
公司基本情况(人民币)
摊薄每股收益 (元）
(066)       081
EPS 增长率 (%)
300       500
NA          80
21          13
一大潜力市场：目前GPU，FPGACPU 仍主导云端的深度学习训练和推理
市场 但通用专用AI 芯片，因性能，成本及耗能优势将渗透部分传统型AI
芯片在云 边，端的市场。我们预估全球AI 云端半导体市场于2019-2024 年
复合成长率应有 36%，边缘运算及终端芯片市场于同期应有 55%增长 远超
过全球市场的7% 整体占全球份额从2019 年的3% 到2024 年的11%。
云 边，端，软件一站式方案的核心竞争力：寒武纪的核心技术是同时具备
最底层的芯片设计，指令集及驱动器，基础系统软件，及加速卡，并使用相
同的自研指令集，让开发者以各类算法完整云、边、端生态系平台的跨越。
寒武纪AI 芯片对比GPU 具有高效能 低成本 低耗能核心竞争力：1 低精
度定点运算的优势；2 AI 推理的优势； 3 算法演进可透过软硬件修改的优
势； 4 运算单元芯片面积较小，却有2 倍以上高效能，50%低耗能优势。
弯道超车的机会：美国商务部于2020 年5 月15 日宣布限制海思在使用美国
半导体EDA 及设备技术来生产半导体 必须取得执照，但我们认为海思昇腾
AI 要申请得到执照有难度，这给了寒武纪一个绝佳机会来弯道超车的机会。
在高毛利云及边缘运算端AI 芯片新产品加持下，寒武纪未来五年营收将大
幅增长CAGR 超过50%，2023 年扭亏转盈，给予买入评级。为了扩大
事业版图，寒武纪在IPO 发4010 万新股，每股6439 元人民币，募集258
亿的资金来补充现金流及用在云端训练及推理，边缘端AI 芯片及系统软件
首家有中科院支持的AI 芯片公司，不但毛利将维持在65%以上，又享受科
创板溢价，比较可比新兴科技公司 平均PS 区间将达40-60 倍，我们目前
用2022 年的30 元每股营收给予50 倍 PS 给估值 一年的目标价为150
终端AI 处理器IP 业务减少的风险，智能计算集群系统事业的风险，同业竞
争的风险，现金流短缺的风险，进入实体清单的风险。
寒 武 纪 (688256SH)   买入（首次评级）
云端及边缘运算AI 芯片及加速卡是两大高毛利增长动能：我们估计未来
五年这两项业务将有80-100%复合增长率的贡献，营收占比从2019 年的
18%继续扩大。主要客户是关联方的中科曙光，并扩大到非关联方的江苏
恒瑞通智能科技，浪潮，联想及北京金山云网络技术。
智能计算集群系统事业不确定：公司智能计算集群系统方面的在手订单包
括横琴先进智能计算平台(二期) 的第二批供货硬件设备，授权软件 合同
金额仅剩下186 亿元，而上半年营收贡献连20 万都没有，除非我们看到
在下半年在手订单大幅回流，今年此业务营收贡献可能不到60%，甚至
连50%占比都可能有问题。
管理费用中的员工股权激励及研发费用的高低，决定亏损是否持续：以
2019 年为例，员工股权激励（股份支付）费用高达944 亿，而543 亿
的研发费用也是超过当年度营收，这些偏高的管理及研发费用，造成公司
2019 年营业亏损，也将决定亏损是否持续。
市场的观点是寒武纪营收及获利会大幅增长；但我们的观点是寒武纪要投
入大量研发，先建立庞大的产品，设计，制程工艺 客户壁垒，短中期获利不易。
7nm 思元290 云端训练芯片的推出（2021 年） 5nm 新产品的研发进度，
智能计算集群系统事业在手订单的回流，管理及研发费用的控制成效，海外重
点客户的建立，及AI 芯片定点运算的突破。
因为缺乏短，中期获利，用市销率PS 来评估寒武纪，较为适当，寒武纪
是国内首家上市的AI 芯片公司，又有中科院的支持及与政府补助，50%以上营
收CAGR 及65%以上毛利率比大多数新兴科技公司高两倍以上，又享受科创板
的溢价，所以其市销率应该比可比公司的20 倍高出甚多，在闭锁期结束之前，
我们认为公司平均PS 区间将达40-60 倍，目前用2022 年的30 元每股营收给
予50 倍 PS 的估值 并给予买入评级，一年的目标价为150 元。
终端AI 处理器IP 业务的减少会影响毛利率；智能计算集群系统事业要是
在手订单没有回流，今年公司营收衰退可期待；很多同业有更多的研发资源投
入，寒武纪将面临更大的价格竞争风险；在客户中科曙光，竞争者海思，依图，
旷视，商汤，云从科技等AI 算法公司，及科大讯飞，云天励飞等AI 平台公司
陆续进入美国商务部工业安全局的实体清单后，我们必须考量寒武纪被列入的
投资要件 2
一、一大潜力市场 5
人工智能平台是工具 不是应用5
二、两大核心竞争力 7
1从云，边缘运算，终端，及系统软件的一站式解决方案 7
2通用型智能芯片对比GPU有高效能，低耗电核心优势 9
三、三个挑战  11
1如何从云端推理发展到云端训练  11
2 扩大战场跟客户抢生意  一个横琴新区采购案占营收近6 成12
3如何缩短设计，制程，软件生态系的差距 13
四、三种主流人工智能演算法 14
五、寒武纪及谷歌的AI 通用芯片将在边缘运算及终端渐成主流 17
六、公司介绍 24
1基本资料 24
2股权结构 25
3募资投入研发 26
4核心客户及供应商的变化27
七、盈利预测及假设 28
1寒武纪营收获利的历史数据及预测的假设基础28
2 给予买入评级及150 元目标价 33
八、主要行业及公司面对的风险 34
图表1：人工智能的多样性 5
图表2：人工智能云，边缘运算，终端半导体及行业市场营收预估5
图表3：国内AI 芯片同比增长率，全球份额，及寒武纪份额6
图表4：寒武纪的AI 芯片技术 8
图表5：寒武纪Neuware 软件架构9
图表6：寒武纪vs 英伟达云端芯片加速卡价格差异比率 10
图表7：人工智能云端推理及训练芯片在不同定点，浮点，精度，峰值比较  11
图表8：横琴先进智能平台及其他AI 集群系统采购细目整理12
图表9：寒武纪智能计算集群系统的硬件，软件架构13
图表10：光掩膜节点升级成本变化13
图表11：台积电制程工艺演进的效能变化比率 14
图表12：人工智能技术工艺的演化14
图表13：卷积输入及输出特征贴图及最大池15
图表14：循环神经机器翻译 16
图表15：深度神经网络 17
图表16：深度学习 17
图表17：各种人工智能半导体优缺点比较 18
图表18：人工智能云端系统图形处理芯片面积 18
图表19：人工智能半导体市场预测以不同芯片种类来分类19
图表20： AI 芯片种类比较表20
图表21：英伟达云端人工智能芯片A100 及系统DGX A100 规格比较表21
图表22：赛灵思BlackLynx与GPU在机器学习推理解决方案的比较22
图表23： 5G 带动不同延迟的人工智能边缘运算的需求 22
图表24：谷歌张量处理器 TPU 3 vs TPU 2 23
图表25：寒武纪主要产品介绍24
图表26：主要产品核心研发领导 25
图表27：寒武纪前10 大股东IPO 前后持股变化 25
图表28：寒武纪原始募资使用计划26
图表29：寒武纪研发项目及进展 26
图表30：寒武纪2017-2019 年前五大客户销售金额及比重变化（万元）27
图表31：寒武纪2017-2019 年前五大供应商采购金额及比重变化（万元） 28
图表32：寒武纪云端智能芯片及加速卡的适配及认证29
图表33：横琴先进智能平台及其他AI 集群系统采购细目整理30
图表34：寒武纪产品营收，同比增长，占比变化图表的历史数据及预测30
图表35：寒武纪各产品线毛利率比较 31
图表36：寒武纪与相关同业毛利率比较31
图表37：寒武纪各营业费用比率及营业利润率预测 32
图表38：政府补助（万元） 32
图表39：寒武纪EPS 与ROE 比较表33
图表40：寒武纪与新兴科技公司利润率及市销率比较33
图表41：寒武纪股价高低区间预测34
人工智能AI 顾名思义就是想用高速计算机运算模式来模拟人脑的认知及推
理，尤其是在收集大量原始数据后，再通过高速计算机利用特殊AI 算法来训练
AI 的认知能力，其中包括视觉（图像，视频），听觉（语言，声音），嗅觉，当
然还有味觉的酸甜苦辣，当AI 高速计算机的认知能力训练完成后，推理算法才
能帮助AI 高速计算机进行，反应，推理，决定。
人工智能平台是工具 不是应用
人工智能平台（包括芯片，模组，软件）在一般人看起来像是一种新型应
用 但在我们看来人工智能芯片在整合软硬件后将成为各种物联网应用的提升效
能工具平台 这就像我们常用的微软Office 软件 微软Office 软件是我们在办公
室应付各种应用的生财工具 因此人工智能平台除了被广泛利用在云端大数据的
深度学习训练和推断外 我们认为人工智能平台也将出现在各式各样的应用端的
边缘运算及终端，从去年英伟达公布的数字来看，早在2016 年，公司就累计
了7 大领域（高等教育，发展工具，互联网，自驾车，金融，政府，生命科学）
及19439 客户使用其深度学习的服务工具 配合软件和之前在云端大数据的深
度学习训练和推断的数据成果库 来达到帮助使用者或取代使用者来执行更佳的
图表1：人工智能的多样性
虽然目前人工智能芯片仍多是传统型芯片，并以昂贵的图形处理器 (GPU)
或以现场可编程门阵列芯片配合中央处理器 (FPGACPU) 为主 来用在云端数
据中心的深度学习训练和推理 但通用专用型AI 芯片，也就是张量处理器或
特定用途集成电路 (ASIC)，主要是针对具体应用场景，固定算法及相同模型的
AI 将在样式类似，数量庞大的云，边缘运算及终端所需推理及训练设备遍地开
花，及逐步渗透部分传统型AI 芯片在云端 边缘运算，及终端的市场，成为人
工智能芯片未来的成长动能 我们预估全球人工智能云端半导体市场于2019-
2024 年复合成长率应有 36%，边缘运算及设备端半导体市场于2019-2024 年
复合成长率应有 55% (请参考图表2) 远超过全球半导体市场在同时间的复合
成长率的7% 整体约占全球半导体市场的份额从2019 年的3% 到2024 年的
图表2：人工智能云，边缘运算，终端半导体及行业市场营收预估
全球半导体市场 (USbn)
410       472        495       535        588
全球半导体市场 (同比)
AI 半导体 (USbn)
18         26          40         51          63
AI 半导体 (同比)
AI IC 佔全球IC 份额 (%)
云端 AI 半导体 (USbn)
123      176       235      300       370
云端 AI 半导体 (同比)
边缘及设备端IC (USbn)
52        84      160      210       260
边缘及设备端 AI IC (同比)
云端 AI 半导体占比 ()
边缘及设备端IC 占比 ()
AI 行业销售额 (USbn)
261       361        490       657        854
AI 行业销售额 (同比)
年达到785 亿人民币的规模预测拿来比较，中国大陆AI 芯片市场的全球份额将
从2020 年的15%，提升到2024 年的18%， 而寒武纪在中国的份额，估计将
从2019 年的 12%， 提升到2024 年的30%或更高。我们认为前瞻产业研究
院预测的中国大陆AI 芯片市场，应该有包括英伟达Nvidia 赛灵思 Xilinx 英特
尔 Intel AI 芯片在中国的销售金额。
图表3：国内AI 芯片同比增长率，全球份额，及寒武纪份额
中国大陆AI芯片同比增长 (%)
寒武紀在中国的份额 %
中国大陆 AI芯片全球份额 %
不同于华为海思虽然在人工智能云，边缘，终端芯片有着领先的地位，但
除了靠华为内部集团的大量采购外，还有无冲突客户的采用，很多华为在5G
通讯，服务器，手机及安防的直接竞争者，还是会以第三方公司像寒武纪设计
的芯片为考量。比特大陆之前也投入大量资金发展云和端的人工智能芯片，但
在吴忌寒与詹克团理念不合分家后，我们估计其人工智能芯片发展将因为资金
短缺而从云转向人工智能在安防端及边缘运算的应用。但就寒武纪而言，公司
始终在人工智能领域耕耘，亦步亦趋， 遥遥领先其他新进。我们以为寒武纪的
两大核心竞争力为整体解决方案及定点算法的坚持。
1从云，边缘运算，终端，及系统软件的一站式解决方案
寒武纪的主营业务是应用于各类云服务器、边缘计算设备、终端设备中人
工智能核心芯片的设计 为客户提供丰富的芯片产品与系统软件解决方案。公司
的主要产品包括终端智能处理器 IP、云端训练及推理智能芯片及加速卡、边缘
智能芯片及加速卡以及与上述产品配套的基础系统软件平台。自 2016 年 3 月
成立以来 公司先后推出了用于终端场景的寒武纪 1A、1H、1M 系列芯片、还
有基于台积电16 纳米制程工艺的思元 100 云端推理和思元 270 云端推理训练
芯片 及其AI 加速卡系列产品，云端训练AI 290 芯片及加速卡，以及基于思元
220 芯片的边缘智能加速卡。其中寒武纪 1A、寒武纪 1H应用于华为的麒麟手
机芯片中 已集成于超过 1 亿台智能手机及其他智能终端设备中 思元系列产品
也已应用于浪潮、联想，中科曙光，滴滴，及海康威视等多家服务器及其相关
厂商的产品中，边缘智能芯片及加速卡的发布标志着公司已形成全面覆盖云端、
边缘端和终端场景的系列化智能芯片产品布局，并广泛应用于手机，IOT、数
据中心、云计算等诸多场景。对于已经建立庞大软（CUDA 应用软件NGC）
硬件(终端：Jatson Nano TX2 Series；边缘运算端：Jet Xavier NX Jet AGX
Xavier Series EGX；云端：Tesla DGX A100 DGX-1Station HGX NGC) 生
态系的英伟达，及华为海思，在使用台积电的7 纳米制程工艺后，在设计上靠
着庞大资源也胜寒武纪一筹，但美国商务部工业安全局5152020 宣布进一步
限制华为海思在使用美国半导体设计软件 EDA 来设计半导体以及利用晶圆代
工所使用的美国半导体设备来生产半导体 必须获得执照，但我们认为，手机
及机顶盒芯片有机会获得执照，但海思的安防，昇腾AI，鲲鹏伺服器CPU，
5G 基地站 ASIC 要申请得到执照可能有困难，这给了寒武纪一个绝佳机会来
我们认为有核心技术的人工智能通用芯片公司必须同时具备芯片（最底层
的硬件物质载体包含高维张量向量传统算术逻辑计算部件），韧体的指令集
及驱动器，基础系统软件（来管理，调用，控制智能芯片来运作），加速卡设计
及测试的能力来完成完整的生态系 寒武纪使用相同的自研指令集与处理器架构
共用相同的基础系统软件平台 实现了云、边、端通用生态的跨越。而开发者可
以研发各类人工智能算法、实现各类人工智能程序 最终实现机器视觉、 语音
处理、自然语言处理以及推荐系统等多样化的人工智能功能。
图表4：寒武纪的AI 芯片技术
指令集的通用性 针对特定场景乃至特定智能算法的加速芯片 这类芯片针
对某个算法实施的硬件化开发 一般不具备指令集或指令集较简单。但寒武
纪研发的通用型AI 芯片，必须具备灵活的指令集来覆盖人工智能领域多
样化的应用场景 (如视觉、语音、自然语言理解、传统机器学习等)。寒武
纪智能芯片的设计思想是通过人工智能算法的计算特征和访存特征来降低
数据搬运的延迟和功耗 支持多个处理器核之间高效并发协作并针对性地
设计更适用于智能算法的数百条处理器基本指令集 与处理器架构配合实现
在人工智能领域内灵活通用的设计目标，不仅需要考虑当前各类智能算法
的特点 也需要对智能算法未来发展的趋势进行预判 从而设计出完备高效
的智能处理器指令集 通过高维张量、向量、逻辑指令等之间的灵活组合来
覆盖对多样化的智能算法 实现人工智能领域内的通用性。举例来说，我们
可以定义硬件的动作，00 是做加法(Add)，01 是做减法(Sub)，10 是做读
取资料(Load) ，11 是做存储资料（Store) 而内部有两个指令暂存器
Instruction register a register b这样软件想做CAB 则会变成读取
register a 的 A，读取 register b 的B， 加register b 的 B 到 register a 中
再存储到 register a 的C 中，而软件可以用高阶语言，让程式员只需要写
CAB  再透过编译器（Complier），转化成上面读取加存储的程式
码，同理，AI 处理器的硬件也定义了一些指令集，其实就是如上面的一些
简单基本动作，可以以软件的方式，组合出各式各样的功能，之后透过编
译器，可以转换为更细碎的指令组合，而硬件就会依照指令的排列顺序，
一个动作一个动作的完成。
处理器架构 寒武纪智能处理器的主功能包含高维张量计算部件、向量计
算部件、传统算术逻辑计算部件 分别用于处理各类智能算法的不同类型操
作。其中高维张量计算部件可对智能算法中核心运算(如卷积运算)进行高
效处理 提升整个处理器的能效。而向量运算部件与算术逻辑计算部件(尤
其后者)则具有更强的灵活性 可对智能算法中频次不高且高维张量无法支
持的运算(如分支跳转等)实现全面覆盖 有力保障了处理器架构的通用性。
基础系统软件 Cambricon Neuware (包含软件开发工具链等)：无须繁琐
的移植即可让同一人工智能应用程序便捷高效地运行在公司云，边，端系
列化芯片与处理器产品之上。在 Cambricon Neuware 的支持下 程序员可
实现跨云边端硬件平台的人工智能应用开发以一处开发、处处运行的
模式大幅提升人工智能应用在不同硬件平台的开发效率和部署速度 同时也
使云，边，端异构硬件资源的统一管理、调度和协同计算成为可能。
Cambricon Neuware 在开发应用时 用户既可以基于 TensorFlow，
PyTorch，Caffe MXNet 等主流编程框架接口编写应用代码 也可以使用公
司预先优化的智能芯片高性能数学库对编程框架算子进行扩展或直接编写
代码 用户同样可以通过智能芯片编程语言(BANG 语言)对算子进行扩展或
直接编写代码 智能芯片编译器可以完成 BANG 语言到 MLU 指令的编译
并在智能芯片核心驱动的支持下使其高效地运行于公司各款芯片产品之上。
在开发过程中 用户还可以通过应用开发调试工具包所提供的调试工具、性
能剖析工具和系统监测 工具等高效地进行应用程序的功能调试和性能调优。
此外 Cambricon Neuware 也可以通过智能芯片虚拟化软件为云计算与数
据中心场景提供关键支撑。
图表5：寒武纪Neuware 软件架构
2通用型智能芯片对比GPU 有高效能，低耗电核心优势
寒武纪虽然在使用台积电的制程工艺上，明显落后于海思最高档AI 昇腾
910 的7nmEUV，AMD 超威Radeon Instinct MI50 及Nvidia 英伟达最新推
出的A100 的7nm，但寒武纪16nm 的思元270 主要对标产品是英伟达价值
2500-2600 美元12nm 的Tesla T4 而不是上万美元的7nm A100，思元 270
可支持 INT16INT8INT4 等多种定点精度计算，INT16 的峰值性能为
64TOPS1（64 万亿次运算），INT8 为 128TOPS，INT4 为 256TOPS。对比
Tesla T4，FP16 的理论峰值性能为 65 TFLOPS，INT8 为 130 TOPS，INT4
为 260 TOPS。思元 270 的功耗为 75w，与 Tesla T4 类似。但所谓的理论峰值
在实测后通常有一定缩水。据阿里云早期核心技术研发人员曾经表示1，T4 在
实测过程中，75w 功耗维持不了多久就降一半频率，而思元270 就能维持相当
的频率。（）我们估计在相同的效能下持续运作，T4 的耗能是思元270 的2 倍
以上，在思元 270 的性能参数展示上，可以看到寒武纪有意强调其定点计算性
能方面的优势，这应该是寒武纪在AI 领域的低精度定点运算有突破，因为低精
度计算的速度和能耗比优势一直受到业界密切关注。而寒武纪7 纳米的思元
290，跟英伟达V100 比较应该也具备2 倍以上高效能，50%低耗能的优势 但
1 来源：寒武纪二代芯片发布在即，独家揭秘如何挑战英伟达 ChainNews
此低成本优势可能要等到寒武纪直接下单台积电，且大量出货达到经济规模才
会展现出来（目前芯片出货量还是非常小，应该连20k 都不到，是使用
除此之外，把GPU 用在深度学习AI 有几个缺点，第一个是深度学习包含
训练和推理两个计算环节，GPU 在深度学习算法并行训练上非常高效，但只能
对于一张输入图像进行推理， 并行度的优势不能完全发挥；第二个是硬件结构
固定不具备可编程性。深度学习算法还未完全稳定，若深度学习算法发生大的
变化，GPU 无法像 FPGA 一样可以灵活的配置硬件结构，也无法像通用AI 能
够针对特殊应用来更改芯片设计；第三个是运算单元芯片面积过大，功耗及成
图表6：寒武纪vs 英伟达云端芯片加速卡价格差异比率
定点运算与浮点运算是计算机计算中最为常用的两种运算表示法，其差异
就体现在定点和浮点上，加减乘除运算都是一样的。定点表示法，即所有位都
表示个位数字，小数点固定；而浮点表示法，则分成两部分，阶码和尾数，尾
数就是数字部分，阶码表示乘幂的大小，也就是小数点位置。所以浮点数在做
运算的时候，除了对尾数做加减乘除，还要处理小数点位置。基于两种不同的
运算表示法规则，导致面对同样长度的定点和浮点运算，浮点计算模式更为复
杂，需要消耗数倍多的功耗及更大的芯片去做运算。但浮点运算又有其不可取
代性。首先，定点表示法运算虽然直观，但是固定的小数点位置决定了固定位
数的整数部分和小数部分，不利于同时表达特别大的数或者特别小的数。而浮
点运算的小数点位置可以移动，运算时不用考虑超出某种数据格式的范围，所
以科学计算法一般都使用浮点。此外，具体到使用 GPU 做训练，业界通常更
倾向于浮点运算单元，主要是因为只有浮点运算才能记录和捕捉到训练时很小
的增量。由于训练的部分模块对精度要求比较高，所以通常必须是高精度的浮
点运算，比如 FP32FP64 （3264 位元的单精度浮点运算）才能搞定。虽然
浮点运算相比定点运算在功耗、计算速度、性价比等方面都不占优势，但截止
目前，浮点计算在云端的训练场景中仍占着主导地位，并且以高精度运算为主。
那么，如何在不增加芯片面积和功耗的前提下，如何大幅提升芯片做训练
的运算能力就成为云端训练芯片的主要研究课题之一。参考计算过程相对简单
的推理计算，目前该领域的 AI 芯片多采用通用AI，ASIC 或低精度浮点运算
GPU，但面对计算过程更为复杂的训练计算，业界一直在尝试是否可能用性价
比更高的定点运算器实现。如何以全部的定点单元（比如 INT8）代替浮点单元，
或者以低精度定点单元配合少量的高精度浮点计算单元（比如 FP32）做更多
的训练任务，目的是达到定点计算的快速度，同时实现接近高精度浮点计算的
精度。目前看来低精度训练确实未必要是浮点数，只要能把数域表达好，0 附
近的小量表达好，什么样的数据表示都可以。
总之，我们判断寒武纪之所以能够大幅度提升低精度训练阶段的计算功耗
比，很有可能是大量采用以定点为主的低精度运算，但要能够成功的切入数据
中心的AI 高精度训练及推理市场，寒武纪除了要发展高精度的浮点运算外，一
套完备成熟的软件生态也是其核心竞争力的重要体现，所以从 2016 年起，寒
武纪逐步推出了NeuWare 软件工具链，该平台终端和云端产品均支持，可以
实现对 TensorFlow、Caffe 和 MXnet 的 API 兼容，同时提供寒武纪专门的高
性能库。英伟达之所以能够在云端训练领域成为绝对主流，其 CUDA 软件生态
的基础功不可没，所以目前 80% 以上的云端加速器是采用英伟达 GPU，而
AMD 的GPUCPU及赛灵思的FPGA 占据非常小的份额。
图表7：人工智能云端推理及训练芯片在不同定点，浮点，精度，峰值比较
思元 100 思元 270-
Tesla T4 V100 GPU A100 GPU
定点INT8 14
32 TOPS 128TOPS 512 TOPS 512TOPS 53 TOPS 130TOPS 60 TOPS
定点INT8 14
128TOPS 512TOPS
1024TFLOPS
8 TFLOPS 20 TFLOPS
作为一个国内最早发AI 专用芯片公司，寒武纪不但要面对客户（华为海思）
上下游整合及调用庞大的手机芯片设计及IP 资源到人工智能芯片的挑战，以及
英伟达在通用AI GPU 加速芯片在制程工艺及CUDA 软件的优势，寒武纪只能
利用少数的资源及现金流，逐步从IP 设计的点，到芯片设计，加速卡设计的线，
一直跨足到系统整合的面，但未来10 年，我们认为寒武纪将面对三个主要挑
战，寒武纪如何克服这些挑战，将是我们建议客户持续关注的重点：1 寒武纪
如何靠着有限的现金流从市场较小的云端推理发展到云端训练？2 寒武纪在一
年内从IP 及芯片设计公司，转变成智能计算集群系统公司，如何解决智能计算
集群系统单一客户横琴新区管理委员会商务局占比过高及行业低毛利，高竞争，
与客户如浪潮，联想，中科曙光争食市场的问题 ？3 寒武纪如何缩短与海思升
腾及英伟达V100A100 GPU 加速卡在IP，设计，制程工艺，软件生态系，定
点 INT8 14 精度理论峰值的技术差距？
1如何从云端推理发展到云端训练
不同于华为海思及英伟达靠着过去强大本业的获利及现金流的积累，可以
投入大量的研发资源，在人工智能领域持续的设计流片，及走到最后的系统整
合。而寒武纪在持续亏损（2019 亏损高达12 亿人民币，净亏损率达266%）
数年后，还要大力追赶，思元270 虽然已经暂时掌握了云端推理用定点低精度
的运算技术，但要顺利在明年推出云端训练用并具有浮点高精度运算的思元
290，寒武纪必须不断烧钱雇用人才，投入庞大研发及流片费用，这次IPO 定
增将筹措28 亿人民币，当然对在手5 亿人民币现金不到的寒武纪而言，具有
稳定财务的作用，但要是年度亏损持续扩大而研发支出不放缓，两年后，我们
不排除寒武纪将卷土重来市场融资，继续摊薄现有股权结构。
2 扩大战场跟客户抢生意  一个横琴新区采购案占营收近6 成
不同于海思跨入云端推理及训练人工智能加速卡，可以有华为这个超大客
户在服务器行业的支持，但寒武纪的主营业务是应用于各类云服务器、边缘计
算设备、终端设备中人工智能核心芯片的设计 为客户提供丰富的芯片产品与系
统软件解决方案，这其中包括终端智能处理器 IP、云端智能推理芯片及加速卡、
边缘智能推理芯片及加速卡以及与上述产品配套的基础系统软件平台，并广泛
应用于手机，IOT、数据中心、云计算等诸多场景；云端智能推理芯片及加速
卡也已应用到国内主流服务器厂商（如浪潮）的产品中 并已实现量产出货 边
缘智能推理芯片及加速卡的发布标志着公司已形成全面覆盖云端，边缘端，终
端的系列化智能芯片产品布局。
但寒武纪为了进一步扩大其营收，于2019 年首度跨入了智能计算集群系
统，于一年内成为寒武纪营收占比67%的主力事业群，就是间接跟直接的销售
整机智能加速系统。从另一角度来看（参考图表），横琴新区管理委员会商务局
于2019 年就占了寒武纪近61%的营收，这其中14%营收是寒武纪销售4000
块思元100 加速卡给中科曙光，然后再出货给横琴新区管理委员会商务局，另
外47%营收是直接销售1300 台智能服务器及48 台并行存储系统给横琴新区
管理委员会商务局，这其中安装了5200 块思元270 加速卡。
图表8：横琴先进智能平台及其他AI 集群系统采购细目整理
思元 270  思元 100 270
18000              2400
思元加速卡单价测算 US
4991              2674
2250                300
8                     8
对寒武纪营收贡献 CNYmn
811                  36
对寒武纪营收占比 (%)
虽然这事业群就跟使用英伟达A100 芯片各种加速卡的服务器系统类似，
但寒武纪总是冒着跟其系统客户抢生意的争议。这就像海思虽然在特定用途的
人工智能云，边缘，终端芯片有着领先的地位，但除了靠华为内部集团的大量
采购外（Atlas 智能计算集群系统），海思的产品要被其他智能计算集群系统客
户的采用，就有相当的难度，很多华为在5G 通讯，服务器，手机的直接竞争
者，还是会以第三方公司像寒武纪设计的芯片及加速卡为考量，但当寒武纪也
跨入其客户将积极发展的人工智能系统领域，跟客户如浪潮，联想，中科曙光
抢生意的争议就逐步浮现。
图表9：寒武纪智能计算集群系统的硬件，软件架构
3如何缩短设计，制程，软件生态系的差距
从寒武纪产品在图表6 与海思及英伟达产品比较表中，我们很清楚的看到
寒武纪在单及双精度浮点运算设计及制程工艺（16nm vs 7nm）都有很大的改
善空间，刚才提到寒武纪若要继续迈入云端训练用浮点高精度运算，就必须不
断烧钱雇用人才，投入庞大研发及流片费用，进行7nm 或更先进的设计。举例
而言，寒武纪若要加快单及双精度浮点运算速度，决定从现在的16nm 跳到
7nm 设计 但光是一个新产品设计流片光掩膜成本就会从4-5 百万美元，增加
到1100 万美元， 这会让一个新产品流片成本暴增一倍以上，所以我们合理推
断，寒武纪必须要等到其年度营业收入超过10 亿人民币后或利用这次的上市
公开发行新股融资来扩大其57 纳米新产品研发动能。举例而言，公司2019 年
的测试化验加工费（主要系研发所用流片费）达124 亿人民币，就占了当年营
而就制程工艺来看，台积电之前曾经公布过以7nm 制程工艺设计的芯片比
16nm 设计的芯片 在耗能上减少达60%，芯片速度增加30%， 每固定单位的
芯片面积可增加233%的晶体管 (Transistors) ，简单来说，不管寒武纪在设计
方面多有竞争力，如果长期要跟海思及英伟达在云端人工智能训练及推理芯片，
加速卡产品的竞争，设计及制程工艺的演进到7 纳米，甚至5 纳米，就是不得
不为的决定，所以寒武纪预期在2021 年顺利推出与海思升腾910 同等级的
7nm 云端AI 训练芯片思元290，要是能顺利量产，就显得非常重要。最后，在
软件生态方面英伟达凭借长久以来的经验积累以及产品推广已形成了较为完善
的CUDA 软件生态系 用户对其产品接受度较高 形成 了一定的用户习惯 而寒
武纪的基础系统软件平台 Cambricon Neuware 的生态完善程度与英伟达相比
图表10：光掩膜节点升级成本变化
图表11：台积电制程工艺演进的效能变化比率
16nmFF to 7nm 7nm to 5nm 5nm to 3nm
四、三种主流人工智能演算法
最早的人工智能出现及运用在19501980 年代，接着转换到1980-2010
年机器学习，从2010 年以后，随着各种演算法CNNs RNNs DNNs 等图影像
视觉学习，辨识，推理的普及，让深入人工智能深入学习的突飞猛进。深度学
习是人工智能和机器学习的一个子集，它使用多层人工神经网络在诸如对象检
测，语音识别，语言翻译等任务中提供最先进的准确性。深度学习与传统的机
器学习技术的不同之处在于，它们可以自动学习图像，视频或文本等数据的表
示，而无需引入手工编码规则或人类领域知识。它们高度灵活的架构可以直接
从原始数据中学习，并在提供更多数据时提高其预测准确性。人工智能的深度
学习近来已经取得的许多突破，例如谷歌 DeepMind 的AlphaGo 及更强大的
AlphaZero 陆续在围棋，西洋棋类比赛夺冠，谷歌Waymo 英伟达的Xavier
及IntelMobileye 的Eye 45 自动驾驶汽车解决方案，亚马逊的Alexa 谷歌的
Google Assistant 苹果Siri，微软的Cortana 及三星的Bixby 智能语音助手等
等。借助加速的深度学习框架，研究人员和数据科学家可以显着加快深度学习
培训，可以从数天或数周的学习缩短到数小时。当模型可以部署时，开发人员
可以依靠人工智能芯片加速的推理平台来实现云，边缘运算设备或自动驾驶汽
车，为大多数计算密集型深度神经网络提供高性能，低延迟的推理。
图表12：人工智能技术工艺的演化
 卷积神经网络CNNs ( Convolutional Neural Networks ) 卷积神经网络
（CNN）是建立在模拟人类的视觉系统，并透过图影像分类模型的突破，
也将是，主要来自于发现可以用于逐步提取图影像内容的更高和更高级别
的表示。 CNN是将图像的原始像素数据作为输入，并学习如何提取这
些特征，并最终推断它们构成的对象。首先，CNN 接收输入特征图：三维
矩阵，其中前两个维度的大小对应于图像的长度和宽度（以像素为单位），
第三维的大小为3（对应于彩色图像的3 个通道：红色，绿色和蓝色）。
CNN 包括一堆模块，每个模块执行三个操作。举例而言，卷积将3x3 过滤
贴图的9 个条件（0，1）套用（先乘后求和以获得单个值）在5x5 输入特
征贴图的9 个像素特征上，而得出3x3 全新的卷积输出特征贴图。在每次
卷积操作之后，会采用最大池演算法（Max pooling），CNN 对卷积特征贴
图进行下采样（以节省处理时间），同时仍保留最关键的特征信息，最大池
化是要从特征贴图上滑动并提取指定大小的图块 (2x2)，对于每个图块，最
大值将输出到新的特征贴图，并丢弃所有其他值。在卷积神经网络的末端
是一个或多个完全连接的层，完全连接的层将一层中的每个神经元连接到
另一层中的每个神经元。 它原则上与多层感知器神经网络（multi-layer
perceptron neural network (MLP) 类似，他们的工作是根据卷积提取的特
征进行分类，CNN 可以包含更多或更少数量的卷积模块，以及更多或更少
的完全连接层，工程师经常试验要找出能够为他们的模型产生最佳结果的
配置。总之，CNN 专门于图影像处理如自动驾驶汽车，安防，人脸辨识，
及疾病图像辨识解决方案。
图表13：卷积输入及输出特征贴图及最大池
 循环神经网络RNNs (Recurrent Neural Network) RNN是一类人工听觉
及说话的神经网络，具有记忆或反馈回路，可以更好地识别数据中的模式。
RNN 是常规人工神经网络的扩展，它增加了将神经网络的隐藏层送回自身
的连接 - 这些被称为循环连接。循环连接提供了一个循环网络，不仅可以
看到它提供的当前数据样本，还可以看到它以前的隐藏状态。具有反馈回
路的循环网络可以被视为神经网络的多个副本，其中一个的输出用作下一
个的输入。与传统的神经网络不同，循环网络使用他们对过去事件的理解
来处理输入向量，而不是每次都从头开始。当正在处理数据序列以进行分
类决策或回归估计时，RNN 特别有用，循环神经网络通常用于解决与时间
序列数据相关的任务。不同于CNN专门于图影像处理，循环神经网络的应
用包括自然语言处理，语音识别，机器翻译，字符级语言建模，图像分类，
图像字幕，股票预测和金融工程。机器翻译是指使用机器将一种语言的源
序列（句子，段落，文档）翻译成相应的目标序列或另一种语言的矢量。
由于一个源句可以以许多不同的方式翻译，因此翻译基本上是一对多的，
并且翻译功能被建模为有条件而非确定性。 在神经机器翻译（NMT）中，
我们让神经网络学习如何从数据而不是从一组设计规则进行翻译。 由于我
们处理时间序列数据，其中语境的上下文和顺序很重要，因此NMT 的首选
网络是递循环神经网络。 可以使用称为注意的技术来增强NMT，这有助
于模型将其焦点转移到输入的重要部分并改进预测过程。举两RNN 的例子，
为了跟踪你的自助餐厅主菜的哪一天，每周在同一天运行同一菜的严格时
间表。如周一的汉堡包，周二的咖喱饭，周三的披萨，周四的生鱼片寿司
和周五的意大利面。使用RNN，如果输出生鱼片寿司被反馈到网络中
以确定星期五的菜肴，那么RNN 将知道序列中的下一个主菜是意大利面
（因为它已经知道有订单而周四的菜刚刚发生，所以星期五的菜是下一个）。
另一个例子是如果我跑了10 英里，需要喝一杯什么？人类可以根据过去的
经验想出如何填补空白。由于RNN的记忆功能，可以预测接下来会发生什
么，因为它可能有足够的训练记忆，类似这样的句子以水结束以完成
图表14：循环神经机器翻译
 深度神经网络DNNs (Deep Neural Network) DNN 在视觉，语言理解和
语音识别等领域取得了关键突破。为了实现高精度，需要大量数据和以后
的计算能力来训练这些网络，但这些也带来了新的挑战。特别是DNN 可能
容易受到分类中的对抗性示例，强化学习中遗忘任务，生成建模中的模式
崩溃的影响以及过长的运算时间。为了构建更好，更强大的基于DNN 的系
统，是能否有效地确定两个神经网络学习的表示何时相同？我们看到的两
个具体应用是比较不同网络学习的表示，并解释DNN 中隐藏层所学习的表
示。设置的关键是将DNN中的每个神经元解释为激活向量，神经元的激活
矢量是它在输入数据上产生的标量输出。例如，对于50 个输入图像，
DNN 中的神经元将输出50 个标量值，编码它对每个输入的响应量。然后，
这50 个标量值构成神经元的激活矢量。因为深度神经网路的规模（即层数
和每层的节点数），学习率，初始权重等众多参数都需要考虑。扫描所有参
数由于时间代价的原因并不可行，因而小批次训练（微型配料），即将多个
训练样本组合进行训练而不是每次只使用一个样本进行训练，被用于加速
模型训练。而最显著的速度提升来自GPU，因为矩阵和向量计算非常适合
使用GPU实现。但使用大规模集群进行深度神经网路训练仍然存在困难，
因而深度神经网路在训练并列化方面仍有提升的空间。
图表15：深度神经网络
五、寒武纪及谷歌的AI 通用芯片将在边缘运算及终端渐成主流
深度学习是一种需要训练的多层次大型神经网络结构（请参考图表16），
其每层节点相当于一个可以解决不同问题的机器学习。利用这种深层非线性的
网络结构，深度学习可以从少数样本展现强大的学习数据集本质特征的能力。
简单来说，深度学习神经网络对数据的处理方式和学习方式与人类大脑的神经
元更加相似和准确。谷歌的阿法狗也是先学会了如何下围棋，然后不断地与自
己下棋，训练自己的深度学习神经网络 更厉害的阿法零 (AlphaZero) 透过更精
准的节点参数 不用先进行预先学习就能自我演化训练学习。深度学习模型需要
通过大量的数据训练才能获得理想的效果 训练数据的稀缺使得深度学习人工智
能在过去没能成为人工智能应用领域的主流算法。但随着技术的成熟，加上各
种行动、固定通讯设备、无人驾驶交通工具 可穿戴科技 各式行动、固定监控
感测系统能互相连接与沟通的亿物联网，骤然爆发的大数据满足了深度学习算
法对于训练数据量的要求。
训练和推理所需要的神经网络运算类型不同。神经网络分为前向传播
（Forward algorithm）其中包括输入层 隐藏层 输出层和后向传播 (Backward
algorithm)主要指的是梯度运算 两者都包含大量并行运算。训练同时需要前向
和后向传播 推理则主要是前向传播。一般而言训练过程相比于推理过程计算量
体更大。云端人工智能系统透过海量的数据集和调整参数优化来负责训练和推
理，边缘运算终端人工智能设备负责推理。推理可在云端进行，也可以在边缘
运算端或设备端进行。等待模型训练完成后，将训练完成的模型（主要是各种
通过训练得到的参数）用于各种应用。应用过程主要包含大量的乘累加矩阵运
算，并行计算量很大，但和训练过程比参数相对固定，不需要大数据支撑，除
在云端实现外，也可以在边缘运算端实现。推理所需参数可由云端训练完毕后，
定期下载更新到应用终端。
图表17：各种人工智能半导体优缺点比较
适用于云计算 图形
较低功耗 低延迟
耗电高，单位成本高 耗电高 浮点速度较
GPU 慢 布线不易
研发期长 初期开发
代表企业 英特尔，超威
赛灵思，Lattice
英特尔Altera
谷歌 亚马逊，华为
海思 寒武纪 比特
大陆 阿里巴巴 百
在深度学习半导体领域里，最重要的是数据和运算。谁的晶体管数量多，
芯片面积大 谁就会运算快和占据优势。因此，在处理器的选择上，可以用于通
用基础计算且运算速率更快的GPU 迅速成为人工智能计算的主流芯片 根据美
国应用材料的公开资料 (请参考图表18) 英伟达的人工智能逻辑芯片配合英特
尔的中央处理器服务器芯片面积达 7432mm2，是不具人工智能的企业用和大
数据服务器的八倍或谷歌专用张量处理器人工智能服务器的三倍多 存储器耗用
面积 (32512mm2) 是其他服务器的三倍以上。可以说，在过去的几年，尤其是
2015 年以来，人工智能大爆发就是由于英伟达公司的图形处理器 得到云端主
流人工智能的应用。但未来因为各个处理器的特性不同 我们认为英伟达的图形
处理器GPU 和谷歌的张量处理器仍能主导通用性云端人工智能深度学习系统
的训练 可编程芯片FPGA 的低功耗及低延迟性应有利于主导云端人工智能深
度学习系统的推理，而特殊用途集成电路 (ASIC) 未来将主导边缘运算及设备端
的训练及推理，但因为成本，运算速度，及耗电优势，也会逐步侵入某些特殊
应用人工智能云端服务器市场，抢下训练及推理运算的一席之地 以下就先列出
各种处理器在云端人工智能系统的优缺点：
图表18：人工智能云端系统图形处理芯片面积
图表19：人工智能半导体市场预测以不同芯片种类来分类
 中央处理器 CPU X86 和ARM 在内的传统CPU处理器架构往往需要数百
甚至上千条指令才能完成一个神经元的处理，但对于并不需要太多的程序
指令，却需要海量数据运算的深度学习的计算需求，这种结构就显得不佳。
中央处理器CPU需要很强的处理不同类型数据的计算能力以及处理分支与
跳转的逻辑判断能力，这些都使得CPU 的内部结构异常复杂 现在CPU
可以达到64bit 双精度，执行双精度浮点源计算加法和乘法只需要13 个
时钟周期，时钟周期频率达到15323gigahertz。CPU 拥有专为顺序逻
辑处理而优化的几个核心组成的串行架构，这决定了其更擅长逻辑控制、
串行运算与通用类型数据运算 当前最顶级的CPU 只有6 核或者8 核，但
是普通级别的GPU 就包含了成百上千个处理单元，因此CPU 对于影像，
视频计算中大量的重复处理过程有着天生的弱势。
图表20： AI 芯片种类比较表
 图形处理器GPU 仍主导云端人工智能深入学习及训练 最初是用在计算机、
工作站、游戏机和一些移动设备上运行绘图运算工作的微处理器 但其海量
数据并行运算的能力与深度学习需求不谋而合，因此，被最先引入深度学
习。GPU 只需要进行高速运算而不需要逻辑判断。GPU 具备高效的浮点
算数运算单元和简化的逻辑控制单元，把串行访问拆分成多个简单的并行
访问，并同时运算。例如，在CPU 上只有20-30%的晶体管(内存存储器
DRAM dynamic random access memory 缓存静态随机存储器Cache
SRAM 控制器 controller 占了其余的70-80% 晶体管) 是用作计算的，但
反过来说 GPU上有70-80%的晶体管是由上千个高效小核心组成的大规模
并行计算架构 (DRAM 和微小的 Cache SRAM controller 占了剩下的20-
30% 晶体管)。大部分控制电路相对简单，且对Cache 的需求小，只有小
部分晶体管来完成实际的运算工作，至于其他的晶体管可以组成各类专用
电路、多条流水线，使得GPU拥有了更强大的处理浮点运算的能力。这决
定了其更擅长处理多重任务，尤其是没有技术含量的重复性工作。不同于
超威及英特尔的GPU 芯片，英伟达的人工智能芯片具有CUDA 的配合软
件是其领先人工智能市场的主要因素。CUDA 编程工具包让开发者可以轻
松编程屏幕上的每一个像素。在CUDA 发布之前， GPU 编程对程序员来
说是一件苦差事，因为这涉及到编写大量低层面的机器码。CUDA 在经过
了英伟达的多年开发和改善之后，成功将Java 或C这样的高级语言开
放给了GPU 编程，从而让GPU 编程变得更加轻松简单，研究者也可以更
快更便宜地开发他们的深度学习模型。因此我们认为目前英伟达配备8 颗
A100 的DGXA100 系统（199000 美元），配合其CUDA 软件及NVLink
快速通道，以16bit 的半精度浮点性能来看，能达到近2480 兆次 (24
petaFLOPS) 深入学习的浮点运算训练速度 若以32bit 的单精度浮点性能
来看，可达到1280 兆次浮点运算（128 petaFLOPS） 目前仍然是云端
人工智能深入学习及训练的最佳通用型解决方案。
图表21：英伟达云端人工智能芯片A100 及系统DGX A100 规格比较表
 现场可编程门阵列芯片FPGA 的优势在低功耗，低延迟性 CPU 内核并不
擅长浮点运算以及信号处理等工作，将由集成在同一块芯片上的其它可编
程内核执行，而GPU与FPGA 都以擅长浮点运算著称。FPGA 和GPU 内
都有大量的计算单元，它们的计算能力都很强。在进行人工智能神经网络
(CNN RNN DNN) 运算的时候，两者的速度会比CPU 快上数十倍以上。
但是GPU 由于架构固定，硬件原来支持的指令也就固定了，而FPGA 则
是可编程的，因为它让软件与应用公司能够提供与其竞争对手不同的解决
方案，并且能够灵活地针对自己所用的算法修改电路。虽然FPGA 比较灵
活 但其设计资源比GPU 受到较大的限制，例如GPU 如果想多加几个核
心只要增加芯片面积就行，但FPGA 一旦型号选定了逻辑资源上限就确定
了。而且，FPGA 的布线资源也受限制，因为有些线必须要绕很远，不像
GPU 这样走ASIC flow 可以随意布线，这也会限制性能。FPGA 虽然在浮
点运算速度 增加芯片面积，及布线的通用性比GPU 来得差，却在延迟性
及功耗上对GPU 有着显着优势。英特尔斥巨资收购Altera 是要让FPGA
技术为英特尔的发展做贡献。表现在技术路线图上，那就是从现在分立的
CPU 芯片分立的FPGA 加速芯片，过渡到同一封装内的CPU 晶片
FPGA 晶片，到最终的集成CPUFPGA 系统芯片。预计这几种产品形式
将会长期共存，因为CPU 和FPGA 的分立虽然性能稍差，但灵活性更高。
目前来看 用于云端的人工智能解决方案是用Xeon CPU 来配合 Nervana
用于云端中间层和边缘运算端设备的低功耗推断解决方案是用Xeon CPU
来配合FPGA 可编程加速卡。赛灵思（Xilinx）于2018 年底推出以低成本，
低延迟，高耗能效率的深度神经网络（DNN）演算法为基础的Alveo 加速
卡 (Alveo U25 U50 U200 U250 U280 采用台积电16nm 制程工艺的
UltraScale FPGA。
图表22：赛灵思BlackLynx 与GPU在机器学习推理解决方案的比较
图表23： 5G 带动不同延迟的人工智能边缘运算的需求
谷歌张量处理器 TPU 3，寒武纪AI 芯片及ASIC  因为它能加速其人工智
能系统TensorFlow的运行，而且效率也大大超过GPU，谷歌的深层神经
网络就是由TensorFlow 引擎驱动的，其第三代张量处理器 (TPU v3
Tensor Processing Unit 大约420 Tera FLOPShp-16bit) 是专为机器学习
由谷歌提供系统设计，博通提供芯片设计服务及智财权专利区块，台积电
提供1612 纳米制程工艺量身定做的，执行每个操作所需的晶体管数量更
少，自然效率更高。TPU 每瓦能为机器学习提供比所有商用GPU 和
FPGA 更高的量级指令。TPU 是为机器学习应用特别开发，以使芯片在
计算精度降低的情况下更耐用，这意味每一个操作只需要更少的晶体管，
用更多精密且大功率的机器学习模型，并快速应用这些模型，因此用户便
能得到更正确的结果。以谷歌子公司深度思考的阿尔法狗及零 (AlphaGo
AlphaZeroDeepMind) 利用人工智能深度学习训练和推理来打败世界各国
排名第一的围棋高手 世界排名第一的西洋棋AI 程式 Stockfish 8 世界排
名第一的日本棋Shogi AI 专家，但我们估计AlphaZero 系统使用至少近5
大排人工智能主机，5000 个张量处理器 1280 个中央处理单元而让云端
的设备异常昂贵。Google TPU 和寒武纪智能芯片的相同点都是通过对人
工智能领域的计算特征和访存特征进行分析和抽象设计出的通用型智能
芯片 其指令集、运算器架构和存储层次都非常适合智能算法 从而让智能
应用上的能效超过了传统CPU 及GPU。Google TPU 和寒武纪智能芯片
的不同点是在处理器架构上采用了不同的路线。Google TPU 的核心是经
典的脉动阵列机技术 脉动阵列本身对于卷积类运算的效率较高 但是对于
相对低频的部分运算操作(如全连接运算、激活运算)的效率不高。对于后
者 Google TPU 引入了额外的硬件单元作为补充。而寒武纪的芯片架构
则直接将算法的基本操作区分为高维张量运算、向量运算、算数逻辑运算
并在处理器中分别通过高维张量、向量、传统算术逻辑等计算部件予以处
理。高维张量计算部件可高效支持卷积运算、全连接运算 而向量计算部
件则可以支持激活等运算 传统算术逻辑计算部件则可以支持分支跳转等。
即使研发期长 初期开发成本高 但国内芯片业者因缺乏先进x86 CPU
GPU 及FPGA 的基础设计智慧财产权（IPs），因此可完全客制化，耗电
量低，性能强的AI 通用IC 设计就立刻成为国内进入人工智能云端及边缘
运算及设备端芯片半导体市场的唯一途径。目前除了比特大陆的算丰
(SOPHON) BM16821684 安防及大数据边缘运算人工智能推理系列产品
已经上市之外，华为海思使用台积电7 纳米制程工艺设计的升腾Ascend
910 系列，号称在16bit 半精度下能达到256 兆次的浮点运算，只有小幅
低于英伟达目前最先进的A100 GPU 的310 兆次的浮点运算解决方案(台
积电7 纳米) 及谷歌之前推出的张量处理器3 的420 兆次的浮点运算解决
方案（台积电1612 纳米）。而国内搜寻引擎龙头百度的昆仑芯片(818-
300 采用Samsung 14 纳米，在Xilinx FPGA 的架构下及150 瓦的功耗下
提供260TOPS 性能)，除了以上这些公司产品外，阿里巴巴的Ali-NPU
及亚马逊的Inferentia (128TOPS) 目前都还没有提供更确实的芯片速度，
耗电量，应用，价格，量产时点，及软件框架规格让我们做出更好的比较
图表24：谷歌张量处理器 TPU 3 vs TPU 2
中科寒武纪科技股份有限公司为中科计算所的博士生导师陈天石于北京成
立于2016 年3 月15 日，陈天石目前担任董事长暨总经理，负责把控公司整体
的技术方向、业务进程以及战略发展方向 并牵头开展学术研究和产业化工作。
截至 2019 年 12 月 31 日 公司总员工数858 人，拥有研发人员 680 人 占员
工总人数的 7925% 拥有硕士及以上学历人员 546 人 占员工总人数的
6364%。截至 2020 年 2 月 29 日 公司已获得授权的专利共计 65 项 其中境
内专利共计 50 项 境外专利共计 15 项。
公司自成立以来，在短短时间内推出了手机AI 用系列IP， 寒武纪1A 1H
1M， 并被海思于2018 年大量采用在其麒麟芯片系列，但在海思于2019 年自
主研发其手机用AI IP 后，寒武纪又能成功的推出云端AI 推理芯片思元100 及
云端AI 推理训练芯片270，加速卡，及设计并外包生产销售智能计算集群系统，
让去年营收同比增长达279%。而今年再叠加思元220 边缘运算芯片及加速卡
的贡献，而明年有思元290 云端训练芯片及加速卡的营收贡献。公司是目前国
内少数几家能够掌握智能芯片及其基础系统软件研发和产品化核心技术的企业
之一 能提供云端，边缘端，终端一体、软硬件协同、训练推理融合、具备统一
生态的系列化智能芯片产品和平台化基础系统软件来处理图像，视频，语音识
别与合成和自然语言。
图表25：寒武纪主要产品介绍
除陈天石外，首席运营官王在是中国科学技术大学计算机应用技术博士学
历，2016 年至 2018 年就职于中科院计算所从事科研工作。2016 年作为公司
创始团队成员加入公司 现任公司董事、副总经理兼首席运营官。而梁军是中国
科学技术大学通信与信息系统硕士学历，于2003 年至 2017 年 就职于华为技
术有限公司基础业务部、并在海思半导体历任工程师、高级工程师、主任工程
师、技术专家、高级技术专家。2017 年起为公司服务 现任公司副总经理兼首
席技术官，总体负责公司的研发工作 总体领导研发团队完成芯片设计与实现、
芯片与板卡产品量产、基础系统软件研发等。
图表26：主要产品核心研发领导
在本次IPO 增资11%股数发行前（从36 亿股增加到40 亿股） 公司控
股股东陈天石直接持有公司 3319%的股份 并作为艾溪合伙的执行事务合伙人
控制艾溪合伙持有公司 851%的股份陈天石合计控制公司 4171% 的股份 为
公司的实际控制人，第二大股东中科算源（为中科院计算所100%持有的子公
司）在IPO 增资后持有164%的寒武纪，而第四大股东苏州工业园区古生代创
业投资主要是南京智子集成电路产业投资企业（6563%）及江苏金财投资有限
公司（3277%）持有的，而第五大股东国投科技成果转化创投基金企业主要系
国家开发投资集团（21%），国家科技风险开发事业中心（20%），宁波梅山保
税港区干平涌顺珞佳熙明投资管理合伙企业（1925%1925%），上海科技
创业投资（10%）所持有。
寒武纪控股股东，实际控制人陈天石，艾溪合伙，艾加溪合伙都承诺在寒
武纪股票上市后，三年内不能转让持股，但其他主要股东如中科算源，古生代
创投，国投基金，南京招商，宁波瀚高等，其闭锁期都是一年，而公司董事，
监事，高级管理人员及核心技术人员都承诺，原则上公司获利后闭锁期为一年，
假如持续亏损，闭锁期最长为三年，因为我们预期公司在未来2-3 年内要扭亏
转盈相对困难，所以整体经营团队应该都是在三年闭锁期的规定之下，但其他
策略投资股东的卖压应该在一年闭锁期之后逐步涌现。
图表27：寒武纪前10 大股东IPO 前后持股变化
寒武纪预期在IPO 用4010 万股，募集将近258 亿人民币的资金，每股定
价在6439 元人民币，略低于招股说明书预期的28 亿资金募集，这其中近7
亿或271%主要用在云端训练芯片及系统软件三年新产品的开发，另外6 亿或
232%用在云端推理芯片及系统软件的开发，还有6 亿或233%用在边缘运算
端AI 芯片及系统软件的开发，当然剩下的68 亿现金是用来补充现金流，这对
去年底手上只剩38 亿现金的寒武纪而言，确实需要。如果看公司目前还在开
发阶段的8 个研发项目，总金额达755 亿，都还是在募资计划的可控范围。当
然除了思元290 云端AI 训练芯片在明年推出外，寒武纪现阶段也已经投入
5nm 制程工艺产品的研发设计，提供支持布 局布线、物理验证、静态时序分析
等全流程设计支持 覆盖所有环节设计需求。保障最新工艺芯片流片一次成功与
图表28：寒武纪原始募资使用计划
图表29：寒武纪研发项目及进展
4核心客户及供应商的变化
2017 年、2018 年和 2019 年公司前五大客户的销售金额合计占营业收入
比例分别 为 10000%、9995%和 9544% 表示客户集中度的风险还是相当高，
而且在2017 及2018 年海思的占比竟高达9834%及9763%， 自从海思自主
研发人工智能IP 及芯片后，2019 年的占比大幅下降到只有1434%的营收。而
智能计算集群系统营收从2018 年的零贡献，一下增加到2019 年的67%，这
主要是从珠海市横琴新区管理委员会商务局（4665%）及西安沣东仪享科技服
务有限公司（1826%）开展的智能计算集群系统项目而来。而第三大客户关联
方中科曙光（1438%）主要系寒武纪云端AI 推理加速卡思元100（采购4000
块思元100 加速卡，安装在1000 台中科曙光的服务器中，然后供给横琴先进
智能计算平台一期项目）的主要客户。中科曙光（603019 CH）是寒武纪第二
大股东中科算源（IPO 后持有寒武纪股份1641%）控制达2063%的企业。而
我们预期2020 年边缘运算AI 芯片及加速卡客户及基础系统软件客户将占有一
席之地， 但智能计算集群系统客户的比重会降低。
图表30：寒武纪2017-2019 年前五大客户销售金额及比重变化（万元）
2017 年-2019 年 公司向前五名直接供应商合计采购的金额占比分别为
9264%、8253%和 6649% 占比虽然高，EDA 工具及IP（主要使用新思科技，
上海国际科学技术有限公司代理Cadence 世芯 Alchip 的EDA 设计工具及IP，
及ARM 的IP）以及晶圆代工（主要应该是透过深圳市朗华供应链服务有限公
司来采购台积电的晶圆代工共乘服务（CyberShuttle prototyping service）及采
购博通Avago 及泰科源所代理的各种电子元器件及半导体），根据产业链了解，
寒武纪是透过博通来进行芯片的设计服务及提供部分高速通讯IP，而晶圆代工
主要就是以台积电为16nm 及7nm 为其主力 芯片 IP 及 EDA 工具主要向
Cadence、Synopsys 和 ARM 等采购 日月光、Amkor 和长电科技是寒武纪的
图表31：寒武纪2017-2019 年前五大供应商采购金额及比重变化（万元）
1寒武纪营收获利的历史数据及预测的假设基础
终端AI 处理器IP 业务逐步淡出：在20172019 年，寒武纪的终端AI
处理器IP 业务主要是让华为海思内建其IP 到其手机麒麟芯片，并占到公
司终端AI 处理器 IP 授权业务销售收入比 例的 10000%、9794%和
9256%，而2019 年终端AI 处理器 IP 授权业务收入相较于 2018 年下滑
4123%，主要系华为海思自研AI 芯片 未与寒武纪继续合作 加上公司短
期内难以开发同等规模的大客户 因此 2020 年公司终端智能处理器 IP 授
权业务收入将继续下滑，公司预期全年1A 加1H IP 授权收入为600-800
万元，第三代IP 1M 授权收入为1000 万元，除了海思外，目前有杭州博
雅鸿图视频技术，厦门星宸科技，展讯通信，北京智芯微电子科技等潜在
客户，但营收占比可能连5 个点占比都没有，我们预期未来这些业务占比
将从2017-2018 的98-100%，降低到去年的15%，到未来几年低于4%
云端及边缘运算AI 芯片及加速卡是两大高毛利增长动能：从2019 年开
始，寒武纪的云端AI 推理思元100 芯片及100270 加速卡，就贡献了
7888 万营收（主要是思元100 加速卡占比98%，思元100 芯片及思元
270 加速卡各占1%），占整体营收比18%，公司预期今年上半年将贡献
了近6300-6500 万营收，虽然同比衰退仍近162-465% 但营收占比大
幅提升到76%，而公司去年底才推出的云端AI 边缘运算芯片思元220 及
其加速卡， 公司就预期今年上半年将贡献近440-530 万营收 占比将达到
6%。再加上2021 年将量产的云端AI 训练芯片思元290 及加速卡，我们
估计未来五年这两项业务将可能有80-100%复合增长率的贡献。寒武纪
去年云端AI 推理芯片及加速卡的主要客户是关联方的中科曙光（占比
809%）及非关联方的江苏恒瑞通智能科技，浪潮，联想，新华三，宝德，
技嘉，长城飞腾及北京金山云网络技术。
图表32：寒武纪云端智能芯片及加速卡的适配及认证
智能计算集群系统事业不稳定，密切关注其风险： 2019 年 寒武纪智能
计算集群系统业务收入主要来源于与珠海市横琴新区管理委员会商务局的
采购案、西安沣东仪享科技服务有限公司开展的智能计算集群系统招标项
目 该等项目占公司智能计算集群系统业务收入比例为 973%，而且在一
年之内从0占比到占总收入的大头达到67%（主要贡献2019 年下半
年）。以横琴先进智能计算平台 (二期)采购项目来看，其实就是寒武纪销
售整机智能加速系统，而这些检测好的系统是委托给中科可控代工将
5200 块思元270 云端推理及训练加速卡及软件整合到透过中科可控代工
的1300 台智能服务器及48 台并行存储系统。但以招股书的最新资料显
示，公司智能计算集群系统方面的在手订单包括横琴先进智能计算平台
(二期) 的第二批供货硬件设备，授权软件 合同金额 仅剩下186 亿元，
而上半年营收贡献连20 万都没有(上海脑科学与类脑研究中心项目优化服
务收入在一季度贡献64 万)，除非我们看到在下半年寒武纪拿到新的地
方政府新基建案及其他在手订单大幅回流，今年此业务营收贡献可能不到
60%，甚至连50%占比都可能有问题。而就长期而言，寒武纪这事业群
直接面对终端使用者客户，如地方数据中心，行业企业和科研机构等，就
需要庞大分散的客户群来稳定在手订单及营收，否则将带给投资人相当大
的营收及获利上下大幅波动的风险。之前也有提到，寒武纪于2019 年跨
入了智能计算集群系统（简单来说，就是自己采购并整合从中科可控来的
服务器设备），但寒武纪总是冒着跟其系统客户抢生意的争议。这就像海
思虽然在特定用途的人工智能云，边缘，终端芯片有着领先的地位，但除
了靠华为内部集团的大量采购外，海思的产品要被其他智能计算集群系统
客户的采用，就有相当的难度，很多华为在5G 通讯，服务器，手机的直
接竞争者，还是会以第三方公司像寒武纪设计的芯片及加速卡为考量，但
当寒武纪也跨入其客户擅长的人工智能系统设计及外包代工领域，跟客户
如浪潮，联想，中科曙光抢生意的疑虑就逐步浮现。
图表33：横琴先进智能平台及其他AI 集群系统采购细目整理
18000              2400
思元加速卡单价测算 US
4991              2674
2250                300
8                     8
对寒武纪营收贡献 CNYmn
811                  36
对寒武纪营收占比 (%)
50%营收复合增长率：受到终端AI 处理器IP 业务大幅衰退（从2019
年15%的营收贡献到2020 年小于4%的营收贡献），加上智能计算集群
系统在手订单不足的影响（从2019 年67%的营收贡献到2020 年小于60%
的营收贡献），我们预估寒武纪2020 年营收同比增长37%到6 亿（公司
在第二轮审核问询函之回复报告中预期69 亿营收），但未来5 年将逐步
缩短与英伟达在AI 芯片技术上的差距下，及其主要竞争者海思因为受到
海思条款的管制（美国商务部工业安全局于5152020 宣布进一步限制华
为海思在使用美国半导体设计软件 EDA 来设计半导体以及使用晶圆代工
所使用的美国半导体设备来生产半导体 必须获取执照），我们预期寒武纪
的云端及边缘运算端推理及训练芯片及加速卡反而有机会打入替代海思的
华为供应链而大幅成长，从而带动寒武纪2020-2024 年的营收复合增长
图表34：寒武纪产品营收，同比增长，占比变化图表的历史数据及预测
营业总收入 (百万人民币)
yy 同比增长 (%)
各产品同比增长 (%)
毛利率的变化：2017 到2019 年度公司综合毛利率分别为 9996%、
9990%及 6819% 毛利率下降原因系 2019 年公司拓展了较低毛利的云
端智能芯片及加速卡、智能计算集群系统业务。但就长期而言，我们估计
998% 毛利率的终端AI 处理器IP 及50-60%毛利率的智能计算集群系统
营收比重会持续下降，而70-80%毛利率的云端边缘AI 芯片及加速卡，
及基础系统软件营收比重会持续提升，这些因素应该会让寒武纪整体毛利
率维持在65-70%。而因为AI 芯片可比标的不多，还有寒武纪仍处于扩大
研发支出增加IP 组合及扩大员工股权激励时期，所以我们拿英伟达及
CEVA 的毛利率做同业比较 而在智能计算集群系统业务方面，我们拿浪
潮及中科曙光做比较，寒武纪高达58%的系统毛利率应该无法长期维持。
图表35：寒武纪各产品线毛利率比较
各产品毛利率 (%)
图表36：寒武纪与相关同业毛利率比较
管理费用中的员工股权激励及研发费用的高低，决定亏损是否持续：我们
从销售（104477 万），管理（632688 万），研发（283 亿）费用中的
职工薪酬中，可以初步计算出公司2019 年每人及每位研发员工的平均薪
资为41-42 万元，跟国内半导体设计行业水平相当，但我们认为管理费用
中的员工股权激励（股份支付）费用及研发费用的高低，决定公司营业亏
损是否持续数年，我们以2019 年为例，员工股权激励（股份支付）费用
高达944 亿，甚至超过当年度营收的444 亿，还好的是员工股权激励
（股份支付）费用不会是每年都发的经常性费用。但2019 年研发费用就
是经常性费用，横跨各种新技术研发如边缘智能芯片，基础系统软件（推
理及训练）， 云端硬件训练平台，云端AI 训练及推理芯片，而543 亿的
研发费用也是超过当年度营收，这些偏高的管理及研发费用，造成公司
2019 年营业亏损达118 亿，营业亏损占营收比率达265%，我们估计未
来两年持续亏损，到2023 年才有机会扭亏转盈。
图表37：寒武纪各营业费用比率及营业利润率预测
财务费用- 净额销售
政府补助扮演重要角色：寒武纪拿到的政府补助主要有二种，一是与资产
相关（政府文件规定用于购建或以其他方式形成长期资产的政府补助划分
为与资产相关的政府补助）的是逐年上升，从2017 年的124 万，暴增到
2018 及2019 年的1228 万及1701 万 二是与收益相关（与收益相关的
政府补助 用于补偿以后期间的相关成本费用或损失的 确认为递延收益
在确认相关成本费用或损失的期间计入当期损益或冲减相关成本）的，
2018 年最高有5686 万 但降到 2019 年的1685 万。整体补助占营收比
从2017 年的105%，2018 年的 59%，降到2019 年的76。
图表38：政府补助（万元）
亏损应还会持续数年：虽然寒武纪于2020 年会因为科创板IPO 增加近
11%的增资新股上市，但因为持续亏损，所以反而减少2020 年摊薄每股
损失，但估计未来2023 年EPS 将扭亏转盈达081，2024 年达252，
ROE 也将从2019 年的27% 回升到2024 年的142%。
图表39：寒武纪EPS 与ROE 比较表
2 给予买入评级及150 元目标价
不像其他的半导体设计行业在IP，布线，制程工艺，市场，韧体指令集，
软件，应用都相对成熟，人工智能芯片设计公司宛如新兴科技行业，未来5 年
全球AI 芯片市场复合增长率高达33%，国内AI 芯片市场复合增长率高达41%，
而寒武纪在国内市场份额目前连3%都不到（目前主要系英伟达，英特尔，赛
灵思等传统芯片商在把持），全球份额连1%都没有，而且必须不断投入大量研
发费用在建立各种新智能算法IP，召募大量研发软件及硬件的设计人才，57
纳米制程工艺产品研发，及多样光掩膜产品流片，大量使用EDA 设计工具并采
买各种现有IP 组合，发展自己的韧体指令集及从云，边缘，到端的生态系软件，
这些都让寒武纪必须不断的扩大研发经费，及利用各种员工股权激励计划来吸
引业界人才，当然数年之后的技术壁垒叠加将让新进者困难重重。
所以短，中期用摊薄每股人民币收益 (EPS)，每股净资产 (Book value per
share)及 ROE 来评估寒武纪都很困难。我们认为用价格对每股营收 （Sales
per share）比或市销率来评估寒武纪，较为适当， 而寒武纪因为毛利率又比大
多数市场上新兴科技公司高数倍，所以其Price to sales 应该比市场高出甚多。
我们以现在股价超过1000 美元的电动车龙头 Tesla 特斯拉 为例， 当时在2010
年上市时，五年平均营业亏损率达71%，毛利率只有20-25%， PS 平均达
20-30 倍，最高达50 倍。谷歌在初上市的前五年，其平均PS 达14 倍，五年
最高平均达19 倍，平均毛利率在60%。英伟达在进入AI 芯片后的2017-2019
年，其平均 PS 达17 倍，最高平均达24 倍，而平均毛利率为61%。亚马逊在
初上市的前三年，其平均PS 达45 倍，五年最高平均达80 倍，平均毛利率在
20%，营业亏损率达23%。
图表40：寒武纪与新兴科技公司利润率及市销率比较
除了可预期的未来营收大幅增长，寒武纪也是国内首家上市的人工智能芯
片公司，又有中科院的支持及与资产及收益相关的政府补助，不但毛利率将维
持在65%以上，又享受科创板公司股数流通少及闭锁期的溢价，所以我们给予
买入评级，我们认为在未来12 个月的闭锁期结束之前，公司平均PS 区间将
达40-60 倍，我们目前用2022 年的3 元每股营收给予50 倍 PS 给估值 我们
摊薄每股人民币收益 (EPS)
给寒武纪一年的目标价为150 元，但在少数股东一年及经营团队持股三年闭锁
期结束后，PS 区间将逐年向下调整。但对于成长性较高的半导体设计行业而
言，投资人给估值多看长期营收获利增长趋势，至少看未来1-2 年的营收及获
利，而至于成长性更高的人工智能芯片设计行业而言，投资人给估值至少看未
来2-3 年的营收及获利，这是为什么我们用2022 年的每股营收预期来定目标
图表41：寒武纪股价高低区间预测
八、主要行业及公司面对的风险
1终端AI 处理器IP 业务减少的风险：刚才提到过从2019 年开始华为海思自研
AI 芯片 未与寒武纪继续合作 而公司短期内难以开发同等规模的大客户 加上
国际巨头英伟达，英特尔，高通，联发科，ARM 的竞争，因此我们预期其
2020 年公司终端智能处理器 IP 授权业务收入将继续下滑，预期未来这些业务
占比将从2017-2018 的98-100%，降低到去年的15%，到未来几年低于4%的
2密切关注智能计算集群系统事业风险：在寒武纪招股书的最新资料显示，公
司智能计算集群系统方面的在手订单包括横琴先进智能计算平台(二期) 的第二
批供货硬件设备，授权软件 合同金额 仅剩下186 亿元，而上半年营收贡献连
20 万都没有，除非我们看到在下半年在手订单大幅回流，今年此业务营收贡献
可能不到60%，甚至连50%占比都可能有问题。且就长期而言，寒武纪这事业
群直接面对终端使用者客户，如地方数据中心，行业企业和科研机构等，就需
要庞大分散的客户群来稳定在手订单及营收，否则将带给投资人相当大的营收
及获利上下大幅波动的风险。
3竞争风险：寒武纪在云端AI 芯片除了目前英伟达的V100 A100，海思的昇
腾910 系列的竞争之外，英特尔，超微，赛灵思，比特大陆也陆续推出相关产
品，而提供AI 处理器IP 的厂商也从ARM， 扩大到CEVA Cadence 等等。而
这些竞争都会带来价格及毛利率的压力。
4现金流短缺风险：寒武纪在持续亏损数年后，还要在设计及制程工艺大力追
赶海思及英伟达，寒武纪必须不断烧钱雇用人才，投入庞大研发及流片费用，
举例而言，寒武纪若要加快单及双精度浮点运算速度，决定从现在的16nm 跳
到7nm 设计 但光是一个新产品设计流片光掩膜成本就会从4-5 百万美元，增
加超过一倍到1100 万美元。这次IPO 定增将筹措258 亿人民币，当然对在手
5 亿人民币现金不到的寒武纪而言，具有稳定财务的作用，但要是年度亏损持
续，两年后，我们不排除寒武纪将卷土重来市场融资，继续摊薄现有股权结构。
而且公司在审核问询函之回复报告书中也确认，未来三年内除募集资金外，仍
需30-36 亿投入研发项目。
5进入实体清单的风险：自从美国商务部将人工智能相关公司科大讯飞，依图，
旷视，商汤，云天励飞，云从等放入采购美国技术需要美国商务部许可的实体
清单后，我们担心上市后的寒武纪（持续采用美国Synopsys Cadence 的
EDA 工具）将成为下一波的目标之一。
附录：三张报表预测摘要
损益表（单位：百万元）
所得税少数股东损益
收益评估 (CNY)
资产负债表（单位：百万
资本总额比率 (%)
固定资产周转率 (x)
总资产周转率 (x)
现金流量表（单位：百万
市场中相关报告评级比率分析说明：
市场中相关报告投资建议为买入得1 分，为
增持得2 分，为中性得3 分，为减持得4
分，之后平均计算得出最终评分，作为市场平均投资建
最终评分与平均投资建议对照：
100  买入； 10120增持 ； 20130中性
30140减持
买入：预期未来612 个月内上涨幅度在15%以上；
增持：预期未来612 个月内上涨幅度在5%15%；
中性：预期未来612 个月内变动幅度在 -5%5%；
减持：预期未来612 个月内下跌幅度在5%以上。
均不得以任何方式对本报告的任何部分制作任何形式的复制、转发、转载、引用、修改、仿制、刊发，或以任何侵
报告进行任何有悖原意的删节和修改。
的资料、意见、预测均反映报告初次公开发布时的判断，在不作事先通知的情况下，可能会随时调整。
操作建议做出任何担保，在任何时候均不构成对任何人的个人推荐。
这些公司正在提供或争取提供多种金融服务。
本报告反映编写分析员的不同设想、见解及分析方法，故本报告所载观点可能与其他类似研究报告的观点及市
此报告仅限于中国大陆使用。