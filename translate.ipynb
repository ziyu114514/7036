{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2a75db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "863d6a8ff8e04e48be5fc1d8016dafff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "730182f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "# import torch\n",
    "\n",
    "# # model_name = \"facebook/nllb-200-3.3B\"\n",
    "# # model_name = 'Helsinki-NLP/opus-mt-zh-en'\n",
    "# model_name = \"facebook/m2m100_1.2B\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map=\"auto\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de606a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_sentences(text, max_chars=300):\n",
    "    sentences = re.split(r\"(?<=[。！？])\", text)\n",
    "    chunks, current = [], \"\"\n",
    "\n",
    "    for s in sentences:\n",
    "        if len(current) + len(s) < max_chars:\n",
    "            current += s\n",
    "        else:\n",
    "            chunks.append(current)\n",
    "            current = s\n",
    "    if current:\n",
    "        chunks.append(current)\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8adfb96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_chunk(chunk):\n",
    "    prompt = f\"\"\"\n",
    "你是一个专业的金融研报翻译助手。请将下面的中文内容翻译成专业英文，要求：\n",
    "- 不要遗漏任何信息\n",
    "- 不要添加解释\n",
    "- 保持金融术语准确\n",
    "- 保持句子结构清晰\n",
    "\n",
    "中文内容：\n",
    "{chunk}\n",
    "\n",
    "英文翻译：\n",
    "\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            temperature=0.2,\n",
    "            top_p=0.9,\n",
    "            do_sample=False,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "\n",
    "        )\n",
    "\n",
    "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # 截取“英文翻译：”之后的部分\n",
    "    return text.split(\"英文翻译：\")[-1].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7df920c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def translate_chunks(chunks, tokenizer, model, batch_size=8):\n",
    "#     results = []\n",
    "#     for i in range(0, len(chunks), batch_size):\n",
    "#         batch = chunks[i:i+batch_size]\n",
    "#         # inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=False).to(\"cuda\")\n",
    "#         inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=False).to(\"cuda\")\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model.generate(\n",
    "#                 **inputs,\n",
    "#                 forced_bos_token_id=tokenizer.get_lang_id(\"en\"),\n",
    "#                 max_new_tokens=256,\n",
    "#                 num_beams=4\n",
    "#             )\n",
    "        \n",
    "\n",
    "\n",
    "#         results.extend([tokenizer.decode(o, skip_special_tokens=True) for o in outputs])\n",
    "#     return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce534525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_chunks(chunks):\n",
    "    results = []\n",
    "    for c in chunks:\n",
    "        results.append(translate_chunk(c))\n",
    "        print(len(c))\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dbf599a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_file(in_path, out_path):\n",
    "    with open(in_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    chunks = split_sentences(text)\n",
    "    translated = translate_chunks(chunks)\n",
    "\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(translated))\n",
    "\n",
    "    print(f\"Translated: {in_path} → {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ea3a6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def translate_file(in_path, out_path, tokenizer, model):\n",
    "#     with open(in_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         text = f.read()\n",
    "\n",
    "#     chunks = split_sentences(text)\n",
    "#     translated_chunks = translate_chunks(chunks, tokenizer, model)\n",
    "\n",
    "#     with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#         f.write(\"\\n\".join(translated_chunks))\n",
    "\n",
    "#     print(f\"Translated: {in_path} → {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6eb8c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# def batch_translate(input_dir, output_dir, tokenizer, model):\n",
    "#     for root, _, files in os.walk(input_dir):\n",
    "#         for file in files:\n",
    "#             if not file.endswith(\".txt\"):\n",
    "#                 continue\n",
    "\n",
    "#             in_path = os.path.join(root, file)\n",
    "#             rel_path = os.path.relpath(root, input_dir)\n",
    "#             out_dir = os.path.join(output_dir, rel_path)\n",
    "#             os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "#             out_path = os.path.join(out_dir, file)\n",
    "#             translate_file(in_path, out_path, tokenizer, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e45d80ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def batch_translate(input_dir, output_dir):\n",
    "    print(model.device)\n",
    "\n",
    "    for root, _, files in os.walk(input_dir):\n",
    "        for file in files:\n",
    "            if not file.endswith(\".txt\"):\n",
    "                continue\n",
    "\n",
    "            in_path = os.path.join(root, file)\n",
    "            rel_path = os.path.relpath(root, input_dir)\n",
    "            out_dir = os.path.join(output_dir, rel_path)\n",
    "            os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "            out_path = os.path.join(out_dir, file)\n",
    "            translate_file(in_path, out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04d6e4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "258\n",
      "278\n",
      "267\n",
      "290\n",
      "32\n",
      "356\n",
      "287\n",
      "276\n",
      "292\n",
      "285\n",
      "232\n",
      "276\n",
      "247\n",
      "167\n",
      "262\n",
      "249\n",
      "289\n",
      "266\n",
      "299\n",
      "212\n",
      "249\n",
      "236\n",
      "183\n",
      "274\n",
      "226\n",
      "296\n",
      "201\n",
      "282\n",
      "289\n",
      "32\n",
      "Translated: reports_txt_by_quarter_cleaned\\2017_Q1\\一汽解放\\深度报告\\20170209_西南证券_一汽解放_业绩反转确定，2017轻装上阵，扬帆起航.txt → reports_txt_by_quarter_cleaned_en\\2017_Q1\\一汽解放\\深度报告\\20170209_西南证券_一汽解放_业绩反转确定，2017轻装上阵，扬帆起航.txt\n",
      "285\n",
      "282\n",
      "265\n",
      "220\n",
      "175\n",
      "253\n",
      "308\n",
      "281\n",
      "202\n",
      "283\n",
      "Translated: reports_txt_by_quarter_cleaned\\2017_Q1\\万华化学\\20170316_群益证券_万华化学_公司产品产销两旺，17业绩同比大增203%.txt → reports_txt_by_quarter_cleaned_en\\2017_Q1\\万华化学\\20170316_群益证券_万华化学_公司产品产销两旺，17业绩同比大增203%.txt\n",
      "261\n",
      "292\n",
      "264\n",
      "112\n",
      "381\n",
      "219\n",
      "286\n",
      "Translated: reports_txt_by_quarter_cleaned\\2017_Q1\\万科A\\20170113_东莞证券_万科A_事件点评：地铁集团入驻 股权纷争缓解 利于重新着力经营.txt → reports_txt_by_quarter_cleaned_en\\2017_Q1\\万科A\\20170113_东莞证券_万科A_事件点评：地铁集团入驻 股权纷争缓解 利于重新着力经营.txt\n",
      "0\n",
      "407\n",
      "263\n",
      "239\n",
      "209\n",
      "317\n",
      "256\n",
      "118\n",
      "Translated: reports_txt_by_quarter_cleaned\\2017_Q1\\万科A\\20170113_国金证券_万科A_深圳地铁强势入主，股权之争或近尾声.txt → reports_txt_by_quarter_cleaned_en\\2017_Q1\\万科A\\20170113_国金证券_万科A_深圳地铁强势入主，股权之争或近尾声.txt\n",
      "160\n",
      "270\n",
      "179\n",
      "273\n",
      "140\n",
      "366\n",
      "245\n",
      "258\n",
      "265\n",
      "276\n",
      "292\n",
      "230\n",
      "277\n",
      "269\n",
      "Translated: reports_txt_by_quarter_cleaned\\2017_Q1\\万科A\\20170113_高华证券_万科A_华润向深铁转让所持万科15%的全部股份，作价人民币370亿元.txt → reports_txt_by_quarter_cleaned_en\\2017_Q1\\万科A\\20170113_高华证券_万科A_华润向深铁转让所持万科15%的全部股份，作价人民币370亿元.txt\n",
      "200\n",
      "295\n",
      "225\n",
      "262\n",
      "285\n",
      "278\n",
      "249\n",
      "278\n",
      "277\n",
      "220\n",
      "225\n",
      "127\n",
      "Translated: reports_txt_by_quarter_cleaned\\2017_Q1\\万科A\\20170116_东兴证券_万科A_事件点评：繁花落尽知归处 待秋日来硕果丰？.txt → reports_txt_by_quarter_cleaned_en\\2017_Q1\\万科A\\20170116_东兴证券_万科A_事件点评：繁花落尽知归处 待秋日来硕果丰？.txt\n",
      "280\n",
      "274\n",
      "268\n",
      "129\n",
      "278\n",
      "571\n",
      "Translated: reports_txt_by_quarter_cleaned\\2017_Q1\\万科A\\20170210_群益证券_万科A_公司1月销售面积创单月新高，超出预期，维持“持有”评级.txt → reports_txt_by_quarter_cleaned_en\\2017_Q1\\万科A\\20170210_群益证券_万科A_公司1月销售面积创单月新高，超出预期，维持“持有”评级.txt\n",
      "287\n",
      "299\n",
      "261\n",
      "293\n",
      "249\n",
      "234\n",
      "134\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m input_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreports_txt_by_quarter_cleaned\u001b[39m\u001b[38;5;124m\"\u001b[39m        \u001b[38;5;66;03m# 中文研报目录\u001b[39;00m\n\u001b[0;32m      2\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreports_txt_by_quarter_cleaned_en\u001b[39m\u001b[38;5;124m\"\u001b[39m    \u001b[38;5;66;03m# 英文研报输出目录\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m batch_translate(input_dir, output_dir)\n",
      "Cell \u001b[1;32mIn[10], line 17\u001b[0m, in \u001b[0;36mbatch_translate\u001b[1;34m(input_dir, output_dir)\u001b[0m\n\u001b[0;32m     14\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(out_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     16\u001b[0m out_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(out_dir, file)\n\u001b[1;32m---> 17\u001b[0m translate_file(in_path, out_path)\n",
      "Cell \u001b[1;32mIn[7], line 6\u001b[0m, in \u001b[0;36mtranslate_file\u001b[1;34m(in_path, out_path)\u001b[0m\n\u001b[0;32m      3\u001b[0m     text \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m      5\u001b[0m chunks \u001b[38;5;241m=\u001b[39m split_sentences(text)\n\u001b[1;32m----> 6\u001b[0m translated \u001b[38;5;241m=\u001b[39m translate_chunks(chunks)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(out_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      9\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(translated))\n",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m, in \u001b[0;36mtranslate_chunks\u001b[1;34m(chunks)\u001b[0m\n\u001b[0;32m      2\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m chunks:\n\u001b[1;32m----> 4\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(translate_chunk(c))\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(c))\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "Cell \u001b[1;32mIn[4], line 18\u001b[0m, in \u001b[0;36mtranslate_chunk\u001b[1;34m(chunk)\u001b[0m\n\u001b[0;32m     15\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 18\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs,\n\u001b[0;32m     20\u001b[0m         max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[0;32m     21\u001b[0m         temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[0;32m     22\u001b[0m         top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m,\n\u001b[0;32m     23\u001b[0m         do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     24\u001b[0m         eos_token_id\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_token_id\n\u001b[0;32m     25\u001b[0m \n\u001b[0;32m     26\u001b[0m     )\n\u001b[0;32m     28\u001b[0m text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# 截取“英文翻译：”之后的部分\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\utils\\_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:2564\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[0;32m   2561\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39muse_cache\n\u001b[0;32m   2563\u001b[0m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[1;32m-> 2564\u001b[0m result \u001b[38;5;241m=\u001b[39m decoding_method(\n\u001b[0;32m   2565\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2566\u001b[0m     input_ids,\n\u001b[0;32m   2567\u001b[0m     logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2568\u001b[0m     stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2569\u001b[0m     generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2570\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgeneration_mode_kwargs,\n\u001b[0;32m   2571\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2572\u001b[0m )\n\u001b[0;32m   2574\u001b[0m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[0;32m   2575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2576\u001b[0m     generation_config\u001b[38;5;241m.\u001b[39mreturn_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2577\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2578\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result\u001b[38;5;241m.\u001b[39mpast_key_values, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_legacy_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2579\u001b[0m ):\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:2787\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   2785\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   2786\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2787\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   2789\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[0;32m   2790\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[0;32m   2791\u001b[0m     outputs,\n\u001b[0;32m   2792\u001b[0m     model_kwargs,\n\u001b[0;32m   2793\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2794\u001b[0m )\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\accelerate\\hooks.py:175\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    173\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 175\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:918\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    917\u001b[0m     return_dict \u001b[38;5;241m=\u001b[39m return_dict_passed\n\u001b[1;32m--> 918\u001b[0m output \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    919\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    920\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:463\u001b[0m, in \u001b[0;36mQwen2ForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n\u001b[0;32m    462\u001b[0m slice_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;241m-\u001b[39mlogits_to_keep, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(logits_to_keep, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m logits_to_keep\n\u001b[1;32m--> 463\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states[:, slice_indices, :])\n\u001b[0;32m    465\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\accelerate\\hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mnew_forward\u001b[39m(module, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 170\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpre_forward(module, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mno_grad:\n\u001b[0;32m    172\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\accelerate\\hooks.py:360\u001b[0m, in \u001b[0;36mAlignDevicesHook.pre_forward\u001b[1;34m(self, module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    352\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    353\u001b[0m             value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    354\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_params_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    355\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdata_ptr() \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_params_map\n\u001b[0;32m    356\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_params_map[value\u001b[38;5;241m.\u001b[39mdata_ptr()]\n\u001b[0;32m    357\u001b[0m         ):\n\u001b[0;32m    358\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_pointers_to_remove\u001b[38;5;241m.\u001b[39madd((value\u001b[38;5;241m.\u001b[39mdata_ptr(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device))\n\u001b[1;32m--> 360\u001b[0m         set_module_tensor_to_device(\n\u001b[0;32m    361\u001b[0m             module,\n\u001b[0;32m    362\u001b[0m             name,\n\u001b[0;32m    363\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device,\n\u001b[0;32m    364\u001b[0m             value\u001b[38;5;241m=\u001b[39mvalue,\n\u001b[0;32m    365\u001b[0m             fp16_statistics\u001b[38;5;241m=\u001b[39mfp16_statistics,\n\u001b[0;32m    366\u001b[0m             tied_params_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_params_map,\n\u001b[0;32m    367\u001b[0m         )\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m send_to_device(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device), send_to_device(\n\u001b[0;32m    370\u001b[0m     kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device, skip_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_keys\n\u001b[0;32m    371\u001b[0m )\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\accelerate\\utils\\modeling.py:343\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[1;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map, non_blocking, clear_cache)\u001b[0m\n\u001b[0;32m    341\u001b[0m             module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m param_cls(new_value, requires_grad\u001b[38;5;241m=\u001b[39mold_value\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m--> 343\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39mnon_blocking)\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    345\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(value, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_dir = \"reports_txt_by_quarter_cleaned\"        # 中文研报目录\n",
    "output_dir = \"reports_txt_by_quarter_cleaned_en\"    # 英文研报输出目录\n",
    "\n",
    "batch_translate(input_dir, output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
