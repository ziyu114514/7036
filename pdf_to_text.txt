import os
import re
from multiprocessing import Pool

import fitz  # PyMuPDF
from PIL import Image

from surya.foundation import FoundationPredictor
from surya.recognition import RecognitionPredictor
from surya.detection import DetectionPredictor
from surya.layout import LayoutPredictor
from surya.settings import settings

# =========================
# 1. 初始化 Surya 模型（全局一次）
# =========================

foundation = FoundationPredictor()
detector = DetectionPredictor()
recognizer = RecognitionPredictor(foundation)
layout_predictor = LayoutPredictor(
    FoundationPredictor(checkpoint=settings.LAYOUT_MODEL_CHECKPOINT)
)

# =========================
# 2. PDF 渲染（高性能版：PyMuPDF）
# =========================

def pdf_to_images_fast(pdf_path, dpi=150):
    """
    使用 PyMuPDF 渲染 PDF 到 PIL.Image 列表。
    """
    doc = fitz.open(pdf_path)
    images = []
    for page in doc:
        try:
            pix = page.get_pixmap(dpi=dpi, annots=False)
            img = Image.frombytes("RGB", (pix.width, pix.height), pix.samples)
            images.append(img)
        except Exception as e:
            print(f"[警告] 渲染失败，跳过该页: {e}")
            continue
    return images

# =========================
# 3. Layout + 文本过滤（只保留正文 Text 区域）
# =========================

def line_in_bbox(line_bbox, layout_bbox):
    x1, y1, x2, y2 = line_bbox
    lx1, ly1, lx2, ly2 = layout_bbox
    # 判断是否有重叠
    return not (x2 < lx1 or x1 > lx2 or y2 < ly1 or y1 > ly2)

def is_in_text_region(line, layout_page):
    """
    只保留被 Layout 标记为 Text 的区域，丢弃 Table、Caption、Figure 等。
    """
    for region in layout_page.bboxes:
        if region.label == "Text" or region.label == "SectionHeader" or region.label == "ListItem":
            if line_in_bbox(line.bbox, region.bbox):
                return True
    return False

# =========================
# 4. 文本清洗 + 碎片过滤
# =========================

EMAIL_RE = re.compile(r"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}")
URL_RE = re.compile(r"(https?://\S+|www\.\S+)")
PHONE_RE = re.compile(r"\d{3,4}-\d{7,8}")

def remove_html(text: str) -> str:
    return re.sub(r"<[^>]+>", "", text)

def is_contact_info(text: str) -> bool:
    if EMAIL_RE.search(text):
        return True
    if URL_RE.search(text):
        return True
    if PHONE_RE.search(text):
        return True
    if any(k in text for k in ["电话", "邮箱", "研究员", "SAC"]):
        return True
    return False

def is_footnote(text: str) -> bool:
    # 脚注/备注
    return text.startswith(("注", "备注", "*"))

def is_page_number(text: str) -> bool:
    return re.fullmatch(r"\d{1,3}", text) is not None

def is_toc_line(text: str) -> bool:
    # 目录中的“...... 12”类似
    if re.search(r"\.{3,}", text):
        return True
    if re.search(r"\d+$", text) and " " in text:
        return True
    return False

def is_header_footer(text: str) -> bool:
    keywords = ["请务必阅读", "重要声明", "证券研究报告", "研究所"]
    return any(k in text for k in keywords)

def is_fragment(text: str) -> bool:
    """
    针对“当前价:”、“8.46 元”、“目标价”等典型碎片做规则过滤。
    """
    # 联系方式
    if is_contact_info(text):
        return True

    # 当前价 / 目标价 / 评级等字段
    if any(k in text for k in ["当前价", "目标价", "买入评级", "维持评级", "上调评级"]):
        return True

    # 单独的数字或价格
    if re.fullmatch(r"[0-9\.\-]+(元|倍)?", text):
        return True

    # 纯数字 + 单位
    if re.fullmatch(r"\d+(\.\d+)?\s*(元|倍|万股|亿元|万股)", text):
        return True

    return False

def is_short_fragment(text: str) -> bool:
    text = text.strip()
    if len(text) <= 2:
        return True
    return False

def is_meta_info(text: str) -> bool:
    """
    丢弃：表格图注、分析师信息、免责声明、资料来源等。
    """
    meta_keywords = [
        "资料来源", "来源", "Wind", "WIND",
        "分析师", "执业证书", "免责声明",
        "请阅读", "重要声明",
        "风险提示",
        "华鑫证券", "证券有限责任公司",
        "投资评级说明", "行业的投资评级说明",
        "华鑫证券有限责任公司", "研究发展部",
        "地址:", "网址:", "邮编:",
    ]
    return any(k in text for k in meta_keywords)

def clean_line_basic(line: str):
    """
    基础行清洗：去空白、页码、目录行、页眉页脚等。
    """
    line = line.strip()
    if not line:
        return None

    if is_page_number(line):
        return None

    if is_toc_line(line):
        return None

    if is_header_footer(line):
        return None

    return line

def clean_text_strict(text: str):
    """
    严格文本清洗：
    - 去 HTML
    - 去联系方式
    - 去脚注
    - 去碎片（当前价/目标价/数字碎片）
    - 去极短行
    - 去资料来源/分析师/免责声明等 meta 信息
    """
    text = remove_html(text).strip()
    if not text:
        return None

    if is_footnote(text):
        return None

    if is_contact_info(text):
        return None

    if is_fragment(text):
        return None

    if is_meta_info(text):
        return None

    if is_short_fragment(text):
        return None

    return text

# =========================
# 5. 单个 PDF 的完整处理：PDF -> clean TXT
# =========================

def pdf_to_text_fast(pdf_path: str) -> str:
    """
    核心要求：
    - 丢弃表格、图注、分析师信息、免责声明、资料来源、当前股价等
    - 不做段落合并，逐行输出正文 Text 区域
    """
    # 1. 渲染 PDF
    images = pdf_to_images_fast(pdf_path, dpi=150)

    if not images:
        return ""

    # 2. 批处理 OCR + Layout
    ocrs = recognizer(images, det_predictor=detector)
    layouts = layout_predictor(images)

    output_lines = []

    for i in range(len(images)):
        ocr_page = ocrs[i]
        layout_page = layouts[i]

        # 3. 遍历 OCR 行，只保留落在 Text 区域的行
        for line in ocr_page.text_lines:
            raw = line.text
            if not raw:
                continue

            # 3.1 只保留正文 Text 区域（丢弃表格、图注等）
            if not is_in_text_region(line, layout_page):
                continue

            # 4. 基础清洗
            basic = clean_line_basic(raw)
            if not basic:
                continue

            # 5. 严格清洗（去当前价/资料来源/分析师/免责声明等）
            strict = clean_text_strict(basic)
            if not strict:
                continue

            # 6. 不做段落合并，逐行输出
            output_lines.append(strict)

    return "\n".join(output_lines)

# =========================
# 6. 多进程批量处理
# =========================

def build_output_path(pdf_path: str, pdf_dir: str, txt_dir: str) -> str:
    """
    复用你原来的目录逻辑：
    - 如果路径中包含“深度报告”：reports_txt/股票名/深度报告/文件.txt
    - 否则：reports_txt/股票名/文件.txt
    """
    rel = os.path.relpath(pdf_path, pdf_dir)
    parts = rel.split(os.sep)

    is_deep = "深度报告" in parts

    if is_deep:
        deep_idx = parts.index("深度报告")
        stock_name = parts[deep_idx - 1] if deep_idx > 0 else "UNKNOWN"
        target_dir = os.path.join(txt_dir, stock_name, "深度报告")
    else:
        stock_name = parts[0] if parts else "UNKNOWN"
        target_dir = os.path.join(txt_dir, stock_name)

    os.makedirs(target_dir, exist_ok=True)
    txt_filename = os.path.splitext(os.path.basename(pdf_path))[0] + ".txt"
    return os.path.join(target_dir, txt_filename)

def process_one_pdf(args):
    pdf_path, pdf_dir, txt_dir = args
    txt_path = build_output_path(pdf_path, pdf_dir, txt_dir)
    print(f"转换中: {pdf_path} → {txt_path}")
    text = pdf_to_text_fast(pdf_path)
    with open(txt_path, "w", encoding="utf-8") as f:
        f.write(text)

def batch_convert_parallel(pdf_dir: str, txt_dir: str, workers: int | None = None):
    pdf_paths = []
    for root, _, files in os.walk(pdf_dir):
        for file in files:
            if file.lower().endswith(".pdf"):
                pdf_paths.append(os.path.join(root, file))

    if not pdf_paths:
        print("未找到任何 PDF 文件")
        return

    if workers is None:
        workers = 1  # GPU 为主，CPU 压力保持很低

    print(f"共找到 {len(pdf_paths)} 个 PDF，使用 {workers} 个进程并行处理。")

    args_list = [(p, pdf_dir, txt_dir) for p in pdf_paths]

    with Pool(workers) as pool:
        pool.map(process_one_pdf, args_list)

if __name__ == "__main__":
    pdf_root = r"East_money_research_report_download\reports_pdf"
    txt_root = r"reports_txt"
    batch_convert_parallel(pdf_root, txt_root)
